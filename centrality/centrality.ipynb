{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centrality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bridge Analysis**\n",
    "- Type 1: Bridges that have followers from both communities\n",
    "  -  Betweeness Centrality :   \n",
    "\n",
    "- Type 2: Bridges that have followers only from one communities, and follow one or more bridges itself.\n",
    "  -  Betweeness Centrality : How critical is the node in spreading information into the community where all its followers are from.\n",
    "  -  Closeness Centrality : How efficient it is for information to spread from this user/node.\n",
    "\n",
    "**Community Analysis**\n",
    "- Closeness Centrality : lower the closeness indicates the more important the node is within its echo chamber. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, '/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset')\n",
    "import header as head\n",
    "import centrality_head as ch\n",
    "\n",
    "centrality_path = \"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/centrality\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge Represent following relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/user_followed_by.json\") as file:\n",
    "    followed_by = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/follow/with_bridges.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_nodes_from(communities[\"rumours\"])\n",
    "rumourEdges = []\n",
    "rumourNodes = communities[\"rumours\"]\n",
    "\n",
    "G.add_nodes_from(communities[\"non_rumours\"])\n",
    "non_rumourEdges = []\n",
    "non_rumourNodes = communities[\"non_rumours\"]\n",
    "\n",
    "G.add_nodes_from(communities[\"bridges\"])\n",
    "bridgeEdges = []\n",
    "bridgeNodes = communities[\"bridges\"]\n",
    "\n",
    "for id in followed_by:\n",
    "    user = followed_by[id]\n",
    "\n",
    "    # if id in communities[\"rumours\"]: rumourNodes.append(id)\n",
    "    # elif id in communities[\"non_rumours\"]: non_rumourNodes.append(id)\n",
    "    # elif id in communities[\"bridges\"]: bridgeNodes.append(id)\n",
    "\n",
    "    for follower in user[\"rumours\"]: \n",
    "        rumourEdges.append((follower,id))\n",
    "        G.add_edge(follower,id)\n",
    "\n",
    "    for follower in user[\"non_rumours\"]: \n",
    "        non_rumourEdges.append((follower,id))\n",
    "        G.add_edge(follower,id)\n",
    "    \n",
    "    for follower in user[\"bridges\"]: \n",
    "        bridgeEdges.append((follower,id))\n",
    "        G.add_edge(follower,id)\n",
    "\n",
    "isolates = list(nx.isolates(G))\n",
    "G.remove_edges_from(isolates)\n",
    "\n",
    "isolates = set(isolates)\n",
    "rumourNodes = set(rumourNodes).difference(isolates)\n",
    "non_rumourNodes = set(non_rumourNodes).difference(isolates)\n",
    "bridgeNodes = set(bridgeNodes).difference(isolates)\n",
    "print(len(G.nodes()))\n",
    "print(len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/react+following.json\") as file:\n",
    "    node_relations = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/follow+react/communities.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "FR_G = ch.makeDiGraph(node_relations,communities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree Centrality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In degree analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree = nx.in_degree_centrality(G)\n",
    "rum = {}\n",
    "non_rum = {}\n",
    "bridges = {}\n",
    "\n",
    "for id in list(in_degree):\n",
    "    if id in communities[\"rumours\"] : rum[id]=in_degree[id]\n",
    "    elif id in communities[\"non_rumours\"] : non_rum[id]=in_degree[id]\n",
    "    elif id in communities[\"bridges\"] : bridges[id]=in_degree[id]\n",
    "\n",
    "# Sort each list \n",
    "rum = dict(sorted(rum.items(), key=lambda item: item[1],reverse=True))\n",
    "non_rum = dict(sorted(non_rum.items(), key=lambda item: item[1],reverse=True))\n",
    "bridges = dict(sorted(bridges.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "in_degree = {\n",
    "    \"rumours\": rum,\n",
    "    \"non_rum\": non_rum,\n",
    "    \"bridges\": bridges\n",
    "}\n",
    "\n",
    "head.writeToJSON(head.makePath([centrality_path,\"degree\"]),\"in_degree\",in_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDegree = ch.degree_centrality(FR_G,\"in\")\n",
    "head.writeToJSON(head.makePath([centrality_path,\"follow+react\"]),\"inDegree\",inDegree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out degree analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_degree = nx.out_degree_centrality(G)\n",
    "rum = {}\n",
    "non_rum = {}\n",
    "bridges = {}\n",
    "\n",
    "for id in list(in_degree):\n",
    "    if id in communities[\"rumours\"] : rum[id]=out_degree[id]\n",
    "    elif id in communities[\"non_rumours\"] : non_rum[id]=out_degree[id]\n",
    "    elif id in communities[\"bridges\"] : bridges[id]=out_degree[id]\n",
    "\n",
    "# Sort each list \n",
    "rum = dict(sorted(rum.items(), key=lambda item: item[1],reverse=True))\n",
    "non_rum = dict(sorted(non_rum.items(), key=lambda item: item[1],reverse=True))\n",
    "bridges = dict(sorted(bridges.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "out_degree = {\n",
    "    \"rumours\": rum,\n",
    "    \"non_rum\": non_rum,\n",
    "    \"bridges\": bridges\n",
    "}\n",
    "\n",
    "head.writeToJSON(head.makePath([centrality_path,\"degree\"]),\"out_degree_2\",out_degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDegree = ch.degree_centrality(FR_G,\"out\")\n",
    "head.writeToJSON(head.makePath([centrality_path,\"follow+react\"]),\"outDegree\",outDegree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweeness centrality analysis\n",
    "\n",
    "The betweeness centrality analysis is based upon non-directed graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_cen = nx.betweenness_centrality(G, k=None, normalized=True, weight=None, endpoints=False, seed=None)\n",
    "rum = {}\n",
    "non_rum = {}\n",
    "bridges = {}\n",
    "\n",
    "for id in list(bet_cen):\n",
    "    if id in communities[\"rumours\"] : rum[id]=bet_cen[id]\n",
    "    elif id in communities[\"non_rumours\"] : non_rum[id]=bet_cen[id]\n",
    "    elif id in communities[\"bridges\"] : bridges[id]=bet_cen[id]\n",
    "\n",
    "# Sort each list \n",
    "rum = dict(sorted(rum.items(), key=lambda item: item[1],reverse=True))\n",
    "non_rum = dict(sorted(non_rum.items(), key=lambda item: item[1],reverse=True))\n",
    "bridges = dict(sorted(bridges.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "new_between = {\n",
    "    \"rumours\": rum,\n",
    "    \"non_rumours\": non_rum,\n",
    "    \"bridges\": bridges\n",
    "}\n",
    "\n",
    "head.writeToJSON(head.makePath([centrality_path,\"betweeness\"]),\"directed_betweenness\",new_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_cen = ch.betweeness_centrality(FR_G)\n",
    "head.writeToJSON(head.makePath([centrality_path,\"follow+react\"]),\"betweeness\",bet_cen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closeness Analysis\n",
    "\n",
    "Type 1 bridges within rumour community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/user_followed_by_v2.json\") as file:\n",
    "    followed_by = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/with_bridges.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "rum_G = nx.DiGraph()\n",
    "\n",
    "rum_G.add_nodes_from(communities[\"rumours\"])\n",
    "rumourEdges = []\n",
    "rumourNodes = communities[\"rumours\"]\n",
    "\n",
    "non_rumourNodes = communities[\"non_rumours\"]\n",
    "\n",
    "rum_G.add_nodes_from(communities[\"bridges\"])\n",
    "bridgeEdges = []\n",
    "bridgeNodes = communities[\"bridges\"]\n",
    "\n",
    "for id in followed_by:\n",
    "    if id in non_rumourNodes: continue\n",
    "\n",
    "    user = followed_by[id]\n",
    "\n",
    "    for follower in user[\"rumours\"]: \n",
    "        rumourEdges.append((follower,id))\n",
    "        rum_G.add_edge(follower,id)\n",
    "    \n",
    "    for follower in user[\"bridges\"]: \n",
    "        bridgeEdges.append((follower,id))\n",
    "        rum_G.add_edge(follower,id)\n",
    "\n",
    "isolates = list(nx.isolates(rum_G))\n",
    "rum_G.remove_edges_from(isolates)\n",
    "\n",
    "isolates = set(isolates)\n",
    "rumourNodes = set(rumourNodes).difference(isolates)\n",
    "bridgeNodes = set(bridgeNodes).difference(isolates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_closeness = nx.closeness_centrality(rum_G)\n",
    "rum = {}\n",
    "bridges = {}\n",
    "\n",
    "for id in list(rumour_closeness):\n",
    "    if id in communities[\"rumours\"] : rum[id]=rumour_closeness[id]\n",
    "    elif id in communities[\"bridges\"] : bridges[id]=rumour_closeness[id]\n",
    "\n",
    "# Sort each list \n",
    "rum = dict(sorted(rum.items(), key=lambda item: item[1],reverse=True))\n",
    "bridges = dict(sorted(bridges.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "rumour_closeness = {\n",
    "    \"rumours\": rum,\n",
    "    \"bridges\": bridges\n",
    "}\n",
    "\n",
    "head.writeToJSON(centrality_path,\"rumour_closeness_2\",rumour_closeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 1 bridges within nonrumour community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/user_followed_by_v2.json\") as file:\n",
    "    followed_by = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/with_bridges.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "non_rum_G = nx.DiGraph()\n",
    "\n",
    "rumourNodes = communities[\"rumours\"]\n",
    "\n",
    "non_rum_G.add_nodes_from(communities[\"non_rumours\"])\n",
    "non_rumourEdges = []\n",
    "non_rumourNodes = communities[\"non_rumours\"]\n",
    "\n",
    "non_rum_G.add_nodes_from(communities[\"bridges\"])\n",
    "bridgeEdges = []\n",
    "bridgeNodes = communities[\"bridges\"]\n",
    "\n",
    "for id in followed_by:\n",
    "    if id in rumourNodes: continue\n",
    "\n",
    "    user = followed_by[id]\n",
    "\n",
    "    for follower in user[\"non_rumours\"]: \n",
    "        non_rumourEdges.append((follower,id))\n",
    "        non_rum_G.add_edge(follower,id)\n",
    "    \n",
    "    for follower in user[\"bridges\"]: \n",
    "        bridgeEdges.append((follower,id))\n",
    "        non_rum_G.add_edge(follower,id)\n",
    "\n",
    "isolates = list(nx.isolates(non_rum_G))\n",
    "non_rum_G.remove_edges_from(isolates)\n",
    "\n",
    "isolates = set(isolates)\n",
    "rumourNodes = set(rumourNodes).difference(isolates)\n",
    "non_rumourNodes = set(non_rumourNodes).difference(isolates)\n",
    "bridgeNodes = set(bridgeNodes).difference(isolates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_rumour_closeness = nx.closeness_centrality(non_rum_G)\n",
    "non_rum = {}\n",
    "bridges = {}\n",
    "\n",
    "for id in list(non_rumour_closeness):\n",
    "    if id in communities[\"non_rumours\"] : non_rum[id]=non_rumour_closeness[id]\n",
    "    elif id in communities[\"bridges\"] : bridges[id]=non_rumour_closeness[id]\n",
    "\n",
    "# Sort each list \n",
    "non_rum = dict(sorted(non_rum.items(), key=lambda item: item[1],reverse=True))\n",
    "bridges = dict(sorted(bridges.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "non_rumour_closeness = {\n",
    "    \"non_rumours\": non_rum,\n",
    "    \"bridges\": bridges\n",
    "}\n",
    "\n",
    "head.writeToJSON(head.makePath([centrality_path,\"closeness\"]),\"non_rumour_closeness\",non_rumour_closeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow+React"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for x,y in FR_G.nodes(data=True):\n",
    "    if y[\"at\"] == \"non_rumours\": nodes.append(x)\n",
    "    elif y[\"at\"] == \"uncategorized\": nodes.append(x)\n",
    "\n",
    "subGraph = FR_G.subgraph(nodes)\n",
    "\n",
    "closeness = ch.closeness_centrality(subGraph)\n",
    "head.writeToJSON(head.makePath([centrality_path,\"follow+react\",\"closeness\"]),\"nonrumours+uncategorized\",closeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connectiveness within rumour community  \n",
    "Labels are given to nodes with more than 2 in-going edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/user_followed_by.json\") as file:\n",
    "    followed_by = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/with_bridges.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "# with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/centrality/degree/in_degree.json\") as file:\n",
    "#     in_degree = json.load(file)\n",
    "\n",
    "rum_community = communities[\"rumours\"]\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(rum_community)\n",
    "\n",
    "for id in rum_community:\n",
    "    try : followers = (followed_by[id])[\"rumours\"]\n",
    "    except: continue\n",
    "    \n",
    "    for f in followers:\n",
    "        G.add_edge(f,id)\n",
    "\n",
    "labels = {}\n",
    "for id in G:\n",
    "    if (G.in_degree[id] > 2) : labels[id] = id+\": \"+str(G.in_degree[id])\n",
    "\n",
    "# pos = nx.spring_layout(G)\n",
    "pos = nx.spring_layout(G, k=0.8)\n",
    "# , iterations=20\n",
    "nx.draw(G,pos,node_size=40)\n",
    "nx.draw_networkx_nodes(G,pos,nodelist=labels.keys(), node_size=60)\n",
    "nx.draw_networkx_labels(G,pos,labels,font_size=13,font_color='r')\n",
    "x_values, y_values = zip(*pos.values())\n",
    "x_max = max(x_values)\n",
    "x_min = min(x_values)\n",
    "x_margin = (x_max - x_min) * 0.25\n",
    "plt.xlim(x_min - x_margin, x_max + x_margin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connectivity within non-rumour community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/user_followed_by.json\") as file:\n",
    "    followed_by = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/with_bridges.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "# with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/centrality/degree/in_degree.json\") as file:\n",
    "#     in_degree = json.load(file)\n",
    "\n",
    "nonrum_community = communities[\"non_rumours\"]\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(nonrum_community)\n",
    "\n",
    "for id in nonrum_community:\n",
    "    try : followers = (followed_by[id])[\"non_rumours\"]\n",
    "    except: continue\n",
    "    \n",
    "    for f in followers:\n",
    "        G.add_edge(f,id)\n",
    "\n",
    "labels = {}\n",
    "for id in G:\n",
    "    if (G.in_degree[id] > 0) : labels[id] = id+\": \"+str(G.in_degree[id])\n",
    "\n",
    "# pos = nx.spring_layout(G)\n",
    "pos = nx.spring_layout(G, k=0.8)\n",
    "# , iterations=20\n",
    "nx.draw(G,pos,node_size=40,node_color=\"green\")\n",
    "nx.draw_networkx_nodes(G,pos,nodelist=labels.keys(), node_size=60)\n",
    "nx.draw_networkx_labels(G,pos,labels,font_size=13,font_color='r')\n",
    "x_values, y_values = zip(*pos.values())\n",
    "x_max = max(x_values)\n",
    "x_min = min(x_values)\n",
    "x_margin = (x_max - x_min) * 0.25\n",
    "plt.xlim(x_min - x_margin, x_max + x_margin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolates = set(isolates)\n",
    "rumourNodes = list(set(rumourNodes).difference(isolates))\n",
    "non_rumourNodes = list(set(non_rumourNodes).difference(isolates))\n",
    "bridgeNodes = list(set(bridgeNodes).difference(isolates))\n",
    "\n",
    "# pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "pos = nx.random_layout(G)\n",
    "node_size = 30\n",
    "\n",
    "# nx.draw_networkx_nodes(G, pos, nodelist=rumourNodes, node_size=node_size, node_color=\"tab:red\")\n",
    "# nx.draw_networkx_nodes(G, pos, nodelist=non_rumourNodes, node_size=node_size, node_color=\"tab:green\")\n",
    "# nx.draw_networkx_nodes(G, pos, nodelist=bridgeNodes, node_size=node_size, node_color=\"tab:blue\")\n",
    "\n",
    "# nx.draw_networkx_edges(\n",
    "#             G,\n",
    "#             pos,\n",
    "#             edgelist=G.out_edges(rumourNodes),\n",
    "#             edge_color=\"tab:red\"\n",
    "#         )\n",
    "# nx.draw_networkx_edges(\n",
    "#             G,\n",
    "#             pos,\n",
    "#             edgelist=G.out_edges(non_rumourNodes),\n",
    "#             edge_color=\"tab:green\"\n",
    "#         )\n",
    "# nx.draw_networkx_edges(\n",
    "#             G,\n",
    "#             pos,\n",
    "#             edgelist=G.in_edges(bridgeNodes),\n",
    "#             edge_color=\"tab:blue\"\n",
    "#         )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.bipartite.gnmk_random_graph(3, 5, 10, seed=123)\n",
    "top = nx.bipartite.sets(G)[0]\n",
    "pos = nx.bipartite_layout(G, top)\n",
    "nx.draw(G,pos)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
