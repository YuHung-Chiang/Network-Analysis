{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset')\n",
    "import header as head\n",
    "\n",
    "bridges_path = \"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges\"\n",
    "type2_path = \"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/type 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Nodes that have followers from both communities. \n",
    "2. Nodes that follows other communites but only has follower from one commmunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained who followed by whom relationship\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/who-follows-whom.json\") as f:\n",
    "    follows = json.load(f)\n",
    "\n",
    "who_followed_by_whom = {}\n",
    "# Find who followed by whom\n",
    "for user in follows:\n",
    "    who_followed_by_whom[user] = []\n",
    "    followed_by = who_followed_by_whom[user]\n",
    "\n",
    "    for f in follows:\n",
    "        if user in follows[f]: followed_by.append(f)\n",
    "\n",
    "# head.writeToJSON(head.getRoot(),\"who_followed_by_whom\",who_followed_by_whom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain all followers from both communities per user \n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/communities.json\") as f:\n",
    "    communities = json.load(f)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/who-follows-whom.json\") as f:\n",
    "    follows = json.load(f)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/who_followed_by_whom.json\") as f:\n",
    "    followed_by = json.load(f)\n",
    "\n",
    "rumours_set = communities[\"rumours\"]\n",
    "non_rumours_set = communities[\"non_rumours\"]\n",
    "uncategorized_set = communities[\"uncategorized\"]\n",
    "\n",
    "bridges = {}\n",
    "ids = []\n",
    "for c in communities.keys():\n",
    "    ids.extend(communities[c])\n",
    "\n",
    "\n",
    "for id in ids:\n",
    "    try:\n",
    "        followers = followed_by[id]\n",
    "    except: \n",
    "        continue\n",
    "    \n",
    "    bridges[id] = {\n",
    "        \"rumours\" : [],\n",
    "        \"non_rumours\" : [],\n",
    "        \"uncategorized\" : []\n",
    "    }\n",
    "    node = bridges[id]\n",
    "\n",
    "    for followee in followers:\n",
    "        if followee in rumours_set: node[\"rumours\"].append(followee)\n",
    "        elif followee in non_rumours_set: node[\"non_rumours\"].append(followee)\n",
    "        elif followee in uncategorized_set: node[\"uncategorized\"].append(followee)\n",
    "    \n",
    "    # uncomment the following if statement to obtain \"user_followed_by_unfiltered\" file\n",
    "    if (len(node[\"rumours\"])==0 and len(node[\"non_rumours\"])==0\n",
    "        and len(node[\"uncategorized\"])==0): bridges.pop(id)\n",
    "\n",
    "head.writeToJSON(bridges_path,\"user_followed_by\",bridges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test whether type 3 bridges affect the identification of type 1 bridge. \n",
    "\n",
    "Result: type 3 bridges does not affect the amount of type 1 bridges being identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/bridges.json\") as file:\n",
    "    b1 = json.load(file)\n",
    "    b1 = b1.keys()\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/bridges_v2.json\") as file:\n",
    "    b2 = json.load(file)\n",
    "    b2 = b2.keys()\n",
    "\n",
    "print(b1==b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update user_followed_by with additional information about bridges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/user_followed_by.json\") as file:\n",
    "    followed_by = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/bridges.json\") as file:\n",
    "    bridges = json.load(file)\n",
    "    bridge_ids = bridges.keys()\n",
    "\n",
    "new_followed_by = {}\n",
    "\n",
    "for f in followed_by:\n",
    "    user = followed_by[f]\n",
    "    new_followed_by[f] = {\n",
    "        \"rumours\":[],\n",
    "        \"non_rumours\":[],\n",
    "        \"uncategorized\":[],\n",
    "        \"bridges\":[]\n",
    "    }\n",
    "\n",
    "    for id in user[\"rumours\"]:\n",
    "        if id in bridge_ids : (new_followed_by[f])[\"bridges\"].append(id)\n",
    "        else : (new_followed_by[f])[\"rumours\"].append(id)\n",
    "    \n",
    "    for id in user[\"non_rumours\"]:\n",
    "        if id in bridge_ids : (new_followed_by[f])[\"bridges\"].append(id)\n",
    "        else : (new_followed_by[f])[\"non_rumours\"].append(id)\n",
    "\n",
    "    for id in user[\"uncategorized\"]:\n",
    "        if id in bridge_ids : (new_followed_by[f])[\"bridges\"].append(id)\n",
    "        else : (new_followed_by[f])[\"uncategorized\"].append(id)\n",
    "\n",
    "head.writeToJSON(bridges_path,\"user_followed_by\",new_followed_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 1 Bridge Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/user_followed_by_v2.json\") as file:\n",
    "    followed_by = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/who-follows-whom.json\") as file:\n",
    "    follows = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/with_bridges.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "type1 = {}\n",
    "for id in followed_by:\n",
    "   user = followed_by[id]\n",
    "   if len(user[\"rumours\"]) > 0 and len(user[\"non_rumours\"]) > 0:\n",
    "       type1[id] = {}\n",
    "       (type1[id])[\"followed_by\"] = followed_by[id]\n",
    "       (type1[id])[\"follows\"] = {\n",
    "            \"rumours\":[],\n",
    "            \"non_rumours\":[],\n",
    "            \"bridges\":[]\n",
    "        }\n",
    "\n",
    "for id in type1:\n",
    "    bridge = type1[id]\n",
    "    following = follows[id]\n",
    "\n",
    "    for f in following:\n",
    "        if f in communities[\"rumours\"]: ((type1[id])[\"follows\"])[\"rumours\"].append(f)\n",
    "        elif f in communities[\"non_rumours\"]: ((type1[id])[\"follows\"])[\"non_rumours\"].append(f)\n",
    "        elif f in communities[\"bridges\"]: ((type1[id])[\"follows\"])[\"bridges\"].append(f)\n",
    "\n",
    "\n",
    "\n",
    "head.writeToJSON(bridges_path,\"type1\",type1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checks whether all type1 bridges tweeted source tweets**  \n",
    "Result: From the analysis below, we can be certain that all type1 bridges tweeted source tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/annotations/tweetId_annotations.json\") as file:\n",
    "    src_tweets = json.load(file)\n",
    "    src_tweets = (src_tweets[\"Source_Tweets\"]).keys()\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/Id-conversions/tweet_to_user.json\") as file:\n",
    "    tweet_to_user = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/type1.json\") as file:\n",
    "    type1 = json.load(file)\n",
    "    type1 = type1.keys()\n",
    "\n",
    "src_ids = []\n",
    "for src in src_tweets:\n",
    "    src_ids.append(tweet_to_user[src])\n",
    "\n",
    "for t in type1:\n",
    "    if t in src_ids : print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checks where does all type1 bridges originate from**  \n",
    "Either from rumour or non-rumour community.\n",
    "\n",
    "Results: \n",
    "- All type1 bridges originated from rumour community.\n",
    "- We can also argue that all type1 bridges spread rumours to non-rumour community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/communities.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/with_bridges.json\") as file:\n",
    "    bridges = json.load(file)\n",
    "    bridges = bridges[\"bridges\"]\n",
    "\n",
    "from_rumour = []\n",
    "from_non_rumour = []\n",
    "\n",
    "for b in bridges:\n",
    "    if b in communities[\"rumours\"]: from_rumour.append(b)\n",
    "    elif b in communities[\"non_rumours\"]: from_non_rumour.append(b)\n",
    "\n",
    "print(\"from_rumour\")\n",
    "for f in from_rumour:\n",
    "    print(f)\n",
    "\n",
    "print(\"from_non_rumour\")\n",
    "for f in from_non_rumour:\n",
    "    print(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining centrality information regarding type 1 bridges' followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/type 1/type1.json\") as file:\n",
    "    type1 = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/centrality/betweeness/directed_betweenness.json\") as file:\n",
    "    betweeness = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/centrality/closeness/rumour_closeness.json\") as file:\n",
    "    rum_closeness = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/centrality/closeness/non_rumour_closeness.json\") as file:\n",
    "    nonrum_closeness = json.load(file)\n",
    "\n",
    "type1_centrality = {}\n",
    "\n",
    "for id in type1:\n",
    "    bridge = type1[id]\n",
    "    type1_centrality[id] = {\n",
    "        \"rum_count\" : len((bridge[\"followed_by\"])[\"rumours\"]),\n",
    "        \"nonrum_count\" : len((bridge[\"followed_by\"])[\"non_rumours\"]),\n",
    "        \"rumours\" : {},\n",
    "        \"non_rumours\" : {},\n",
    "    }\n",
    "\n",
    "    rum_followers = (type1_centrality[id])[\"rumours\"]\n",
    "    nonrum_followers = (type1_centrality[id])[\"non_rumours\"]\n",
    "\n",
    "    for user in (bridge[\"followed_by\"])[\"rumours\"]:\n",
    "        betw = (betweeness[\"rumours\"])[user]\n",
    "        close = (rum_closeness[\"rumours\"])[user]\n",
    "        if betw > 0 or close > 0:\n",
    "            rum_followers[user] = {\n",
    "                \"betweeness\":(betweeness[\"rumours\"])[user],\n",
    "                \"closeness\":(rum_closeness[\"rumours\"])[user]\n",
    "            }\n",
    "    \n",
    "    for user in (bridge[\"followed_by\"])[\"non_rumours\"]:\n",
    "        betw = (betweeness[\"non_rumours\"])[user]\n",
    "        close = (nonrum_closeness[\"non_rumours\"])[user]\n",
    "        if betw > 0 or close > 0:\n",
    "            nonrum_followers[user] = {\n",
    "                \"betweeness\":(betweeness[\"non_rumours\"])[user],\n",
    "                \"closeness\":(nonrum_closeness[\"non_rumours\"])[user]\n",
    "            }\n",
    "\n",
    "head.writeToJSON(head.makePath([bridges_path,\"type 1\"]),\"centrality_2\",type1_centrality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 2 Bridge Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain all users whom followers are all from the same community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/user_followed_by_v2.json\") as file:\n",
    "    followed_by = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/who-follows-whom.json\") as file:\n",
    "    follows = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/with_bridges.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "type2 = {}\n",
    "for id in followed_by:\n",
    "    user = followed_by[id]\n",
    "    if len(user[\"rumours\"]) == 0 or len(user[\"non_rumours\"]) == 0:\n",
    "        if len(user[\"rumours\"]) == 0 and len(user[\"non_rumours\"]) == 0:\n",
    "            continue\n",
    "        \n",
    "        else :\n",
    "            type2[id] = {}\n",
    "            (type2[id])[\"followed_by\"] = followed_by[id]\n",
    "            (type2[id])[\"follows\"] = {\n",
    "                \"rumours\":[],\n",
    "                \"non_rumours\":[],\n",
    "                \"bridges\":[]\n",
    "            }\n",
    "\n",
    "for id in type2:\n",
    "    bridge = type2[id]\n",
    "    following = follows[id]\n",
    "\n",
    "    for f in following:\n",
    "        if f in communities[\"rumours\"]: ((type2[id])[\"follows\"])[\"rumours\"].append(f)\n",
    "        elif f in communities[\"non_rumours\"]: ((type2[id])[\"follows\"])[\"non_rumours\"].append(f)\n",
    "        elif f in communities[\"bridges\"]: ((type2[id])[\"follows\"])[\"bridges\"].append(f)\n",
    "\n",
    "\n",
    "head.writeToJSON(type2_path,\"general\",type2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 2 bridges that follows another community from its own followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/type 2/general.json\") as file:\n",
    "    gen_type2 = json.load(file)\n",
    "\n",
    "follows_other_community = {}\n",
    "follows_other_community.clear()\n",
    "\n",
    "for type2 in gen_type2:\n",
    "    bridge = gen_type2[type2]\n",
    "    followees = bridge[\"followed_by\"]\n",
    "    following = bridge[\"follows\"]\n",
    "    if len(followees[\"rumours\"])!=0 and len(following[\"non_rumours\"]) !=0: \n",
    "        follows_other_community[type2] = gen_type2[type2]\n",
    "    elif len(followees[\"non_rumours\"])!=0 and len(following[\"rumours\"]) !=0: \n",
    "        follows_other_community[type2] = gen_type2[type2]\n",
    "    # else: follows_other_community[type2] = gen_type2[type2]\n",
    "\n",
    "head.writeToJSON(type2_path,\"follows_other_community\",follows_other_community)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 2 bridges that follows other bridges\n",
    "Result:\n",
    "- Most type 2 bridges only have followers from rumour and bridge communities. \n",
    "- This shows that the traverse of information from rumour to non-rumour community and vice versa is only acheived by type 1 bridges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/type 2/general.json\") as file:\n",
    "    gen_type2 = json.load(file)\n",
    "\n",
    "follows_bridge = {}\n",
    "for type2 in gen_type2:\n",
    "    bridge = gen_type2[type2]\n",
    "    following = bridge[\"follows\"]\n",
    "    if len(following[\"bridges\"]) != 0:\n",
    "        # non-relaxed if statement\n",
    "        if (len(following[\"rumours\"]) == 0 and len(following[\"non_rumours\"]) == 0):\n",
    "            follows_bridge[type2] = gen_type2[type2]\n",
    "\n",
    "head.writeToJSON(type2_path,\"follows_bridges\",follows_bridge)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
