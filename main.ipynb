{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following are analysis performed for this Bachelor Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tabulate import tabulate\n",
    "from texttable import Texttable\n",
    "import latextable\n",
    "import header as head\n",
    "import json\n",
    "import os\n",
    "\n",
    "annotations_path = \"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/annotations\"\n",
    "community_path = \"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection\"\n",
    "bridges_path = \"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges\"\n",
    "figure_path = \"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/Figures\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 74 rumour source tweet with 0 non-rumour source tweet within the total of 74 rumour tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = head.getPath()\n",
    "# print(p)\n",
    "head.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging following relationship into a JSON file\n",
    "head.followRelate()\n",
    "\n",
    "with open('/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/relateIds.json') as f:\n",
    "    data = json.load(f)\n",
    "    followRelate_Ids = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dictionary for tweetId and userId conversion\n",
    "# results are saved in Id-conversions folder\n",
    "head.gen_idConversion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are followers who reacted to the source tweets. \n",
    "* reacts from followers are saved in `react_follows`\n",
    "* reacts from non-followers are saved in `react_not_follows`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See whether there are any reactors that follows the person they responded to \n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/relateIds.json\") as f:\n",
    "    followingRelate = json.load(f)\n",
    "\n",
    "head.is_follower_react(followingRelate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/with_bridges.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/who-follows-whom.json\") as file:\n",
    "    who_follows_whom = json.load(file)\n",
    "\n",
    "bridges = communities[\"bridges\"]\n",
    "rumours = communities[\"rumours\"]\n",
    "non_rumours = communities[\"non_rumours\"]\n",
    "\n",
    "bridges_follow = {}\n",
    "\n",
    "for bridge in bridges:\n",
    "    follows = who_follows_whom[bridge]\n",
    "    bridges_follow[bridge] = {\n",
    "        \"rumours\":[],\n",
    "        \"non_rumours\":[],\n",
    "        \"bridges\":[]\n",
    "    }\n",
    "\n",
    "    for f in follows:\n",
    "        if f in rumours: (bridges_follow[bridge])[\"rumours\"].append(f)\n",
    "        elif f in non_rumours: (bridges_follow[bridge])[\"non_rumours\"].append(f)\n",
    "        elif f in bridges: (bridges_follow[bridge])[\"bridges\"].append(f)\n",
    "\n",
    "head.writeToJSON(head.getRoot(),\"bridges_follows\",bridges_follow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/annotations/tweetId_annotations.json\") as file:\n",
    "    tweets_type = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/Id-conversions/tweet_to_user.json\") as file:\n",
    "    tweet_to_user = json.load(file)\n",
    "\n",
    "tweets_type_by_userId = {\n",
    "    \"Source_Tweets\": {},\n",
    "    \"Reply_Tweets\": {}\n",
    "}\n",
    "\n",
    "for id in tweets_type[\"Source_Tweets\"]:\n",
    "    user = tweet_to_user[id]\n",
    "    try: (tweets_type_by_userId[\"Source_Tweets\"])[user] = (tweets_type_by_userId[\"Source_Tweets\"])[user]+1\n",
    "    except : (tweets_type_by_userId[\"Source_Tweets\"])[user] = 1\n",
    "    # tweets_type_by_userId[\"Source_Tweets\"].append(tweet_to_user[id])\n",
    "\n",
    "# tweets_type_by_userId[\"Source_Tweets\"] = list(set(tweets_type_by_userId[\"Source_Tweets\"]))\n",
    "\n",
    "for id in tweets_type[\"Reply_Tweets\"]:\n",
    "    user = tweet_to_user[id]\n",
    "    try: (tweets_type_by_userId[\"Reply_Tweets\"])[user] = (tweets_type_by_userId[\"Reply_Tweets\"])[user]+1\n",
    "    except : (tweets_type_by_userId[\"Reply_Tweets\"])[user] = 1\n",
    "    # tweets_type_by_userId[\"Reply_Tweets\"].append(tweet_to_user[id])\n",
    "\n",
    "# tweets_type_by_userId[\"Reply_Tweets\"] = list(set(tweets_type_by_userId[\"Reply_Tweets\"]))\n",
    "head.writeToJSON(annotations_path,\"tweets_type_by_userId\",tweets_type_by_userId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reacting Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reacted_by = head.who_reacted_by_whom()\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/communities.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "rumours = communities[\"rumours\"]\n",
    "non_rumours = communities[\"non_rumours\"]\n",
    "uncategorized = communities[\"uncategorized\"]\n",
    "\n",
    "cat_who_reacted_by_whom = {}\n",
    "\n",
    "for id in reacted_by:\n",
    "    if not(id in rumours or id in non_rumours or id in uncategorized):\n",
    "        continue\n",
    "    cat_who_reacted_by_whom[id] = {}\n",
    "    user = cat_who_reacted_by_whom[id]\n",
    "    user[\"rumours\"] = list(set(reacted_by[id]).intersection(set(rumours)))\n",
    "    user[\"non_rumours\"] = list(set(reacted_by[id]).intersection(set(non_rumours)))\n",
    "    user[\"uncategorized\"] = list(set(reacted_by[id]).intersection(set(uncategorized)))\n",
    "\n",
    "    # pop keys with no reactants\n",
    "    if not(user[\"rumours\"] or user[\"non_rumours\"] or user[\"uncategorized\"]):\n",
    "        cat_who_reacted_by_whom.pop(id)\n",
    "\n",
    "head.writeToJSON(head.getRoot(),\"who_reacted_by_whom\",cat_who_reacted_by_whom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out type1 bridges\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/react/type1.json\") as file:\n",
    "    react_type1 = json.load(file)\n",
    "    react_type1 = react_type1.keys()\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/who_reacted_by_whom.json\") as file:\n",
    "    who_follows_whom = json.load(file)\n",
    "\n",
    "for key, values in who_follows_whom.items():\n",
    "    bridges = []\n",
    "    for id in values[\"rumours\"]:\n",
    "        if id in react_type1:\n",
    "            bridges.append(id)\n",
    "            values[\"rumours\"].remove(id)\n",
    "    \n",
    "    for id in values[\"non_rumours\"]:\n",
    "        if id in react_type1:\n",
    "                bridges.append(id)\n",
    "                values[\"non_rumours\"].remove(id)\n",
    "    \n",
    "    for id in values[\"uncategorized\"]:\n",
    "        if id in react_type1:\n",
    "            bridges.append(id)\n",
    "            values[\"uncategorized\"].remove(id)\n",
    "    \n",
    "    (who_follows_whom[key])[\"bridges\"] = bridges\n",
    "\n",
    "head.writeToJSON(head.getRoot(),\"who_reacted_by_whom\",who_follows_whom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge following and reacting relationships into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/who_reacted_by_whom.json\") as file:\n",
    "    reacting_relate = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/user_followed_by.json\") as file:\n",
    "    following_relate = json.load(file)\n",
    "\n",
    "react_ids = reacting_relate.keys()\n",
    "follow_id = following_relate.keys()\n",
    "\n",
    "# Find left over \n",
    "duplicates_id = list(set(follow_id).intersection(set(react_ids)))\n",
    "follow_id = list(set(follow_id).difference(set(react_ids)))\n",
    "\n",
    "for key, values in reacting_relate.items():\n",
    "    if not(key in duplicates_id) : continue\n",
    "    \n",
    "    values[\"rumours\"] = list(set(values[\"rumours\"]).union(set((following_relate[key])[\"rumours\"])))\n",
    "    values[\"non_rumours\"] = list(set(values[\"non_rumours\"]).union(set((following_relate[key])[\"non_rumours\"])))\n",
    "    values[\"bridges\"] = list(set(values[\"bridges\"]).union(set((following_relate[key])[\"bridges\"])))\n",
    "    values[\"uncategorized\"] = list(set(values[\"uncategorized\"]).union(set((following_relate[key])[\"uncategorized\"])))\n",
    "\n",
    "for follow in follow_id:\n",
    "    reacting_relate[follow] = following_relate[follow]\n",
    "\n",
    "head.writeToJSON(head.getRoot(),\"react+following\",reacting_relate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove bridge nodes from non-bridge followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/follow+react/communities.json\") as file:\n",
    "    FR_communities = json.load(file)\n",
    "    bridges = FR_communities[\"bridges\"]\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/react+following.json\") as file:\n",
    "    FR_relations = json.load(file)\n",
    "\n",
    "for key, values in FR_relations.items():\n",
    "    for id in values[\"rumours\"]:\n",
    "        if id in bridges: values[\"rumours\"].remove(id); values[\"bridges\"].append(id)\n",
    "    for id in values[\"non_rumours\"]:\n",
    "        if id in bridges: values[\"non_rumours\"].remove(id); values[\"bridges\"].append(id)\n",
    "    for id in values[\"uncategorized\"]:\n",
    "        if id in bridges: values[\"uncategorized\"].remove(id); values[\"bridges\"].append(id)\n",
    "\n",
    "head.writeToJSON(head.getRoot(),\"react+following\",FR_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many users are identified\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/annotations/tweetId_annotations.json\") as file:\n",
    "    annotations = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/Id-conversions/tweet_to_user.json\") as file:\n",
    "    tweet_to_id = json.load(file)\n",
    "\n",
    "users = set()\n",
    "for id, types in annotations.items():\n",
    "    tweets = types.keys()\n",
    "    for t in tweets:\n",
    "        users.add(tweet_to_id[t])\n",
    "\n",
    "print(len(users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of users from each community \n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/communities.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "for id, values in communities.items():\n",
    "    print(id+\": \"+str(len(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/communities.json\") as file:\n",
    "    communities = json.load(file)\n",
    "\n",
    "for id, values in communities.items():\n",
    "    entries = [[' '.join([id,\"ids\"]),\"\",\"\",\"\",\"\",\"\"]]\n",
    "    ids = []\n",
    "\n",
    "    for v in values:\n",
    "        ids.append(v)\n",
    "        if len(ids)==6 : \n",
    "            entries.extend([ids])\n",
    "            ids = []\n",
    "    \n",
    "    if len(ids) >0 and len(ids) < 6:\n",
    "        for i in range(6-len(ids)): ids.append(\"\")\n",
    "        entries.extend([ids])\n",
    "    \n",
    "    head.writeToTxt(head.makePath([figure_path]),' '.join([id,\"ids\"])+\"2\",head.makeTable([\"c\",\"c\",\"c\",\"c\",\"c\",\"c\"],[\"t\",\"t\",\"t\",\"t\",\"t\",\"t\"],[\"i\",\"i\",\"i\",\"i\",\"i\",\"i\"],entries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table for type 1 bridges\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/follow/type 1/type1.json\") as file:\n",
    "    type1 = json.load(file)\n",
    "\n",
    "header = [\"id\"]+[\"\" for i in range(4)]\n",
    "entries = [header]\n",
    "inter_data = []\n",
    "for id in type1:\n",
    "    inter_data.append(id)\n",
    "    if len(inter_data) == len(header): \n",
    "        entries.append(inter_data)\n",
    "        inter_data = []\n",
    "\n",
    "if len(inter_data) > 0: inter_data.extend([\"\" for i in range(len(header)-len(inter_data))])\n",
    "entries.append(inter_data)\n",
    "\n",
    "cols_align = [\"c\" for i in range(len(header))]\n",
    "cols_valign = [\"t\" for i in range(len(header))]\n",
    "cols_dtype = [\"i\" for i in range(len(header))]\n",
    "\n",
    "head.writeToTxt(head.makePath([figure_path,\"follow\",\"type 1\"]),\"type1_ids\",head.makeTable(cols_align,cols_valign,cols_dtype,entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table for the number of each type of tweet a bridge tweeted\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/follow/type 1/tweets_type.json\") as file:\n",
    "    tweets_type = json.load(file)\n",
    "\n",
    "header = [\"id\",\"\\# source tweet\",\"\\# reply tweet\"]\n",
    "entries = [header]\n",
    "\n",
    "for id, types in tweets_type.items():\n",
    "    inter_data = [id]\n",
    "    for i, num in types.items():\n",
    "        inter_data.append(num)\n",
    "    \n",
    "    entries.append(inter_data)\n",
    "\n",
    "cols_align = [\"c\" for i in range(len(header))]\n",
    "cols_valign = [\"t\" for i in range(len(header))]\n",
    "cols_dtype = [\"i\" for i in range(len(header))]\n",
    "\n",
    "head.writeToTxt(head.makePath([figure_path,\"follow\",\"type 1\"]),\"tweets_type\",head.makeTable(cols_align,cols_valign,cols_dtype,entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow + follower counts\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/type 1/centrality.json\") as file:\n",
    "    bridge_centrality = json.load(file)\n",
    "\n",
    "table = Texttable()\n",
    "table.set_cols_align([\"c\", \"c\", \"c\"])\n",
    "table.set_cols_valign([\"t\", \"m\", \"b\"])\n",
    "table.add_rows([[\"id\",\"#rumour followers\",\"#non-rumour followers\"]])\n",
    "\n",
    "data = []\n",
    "for id in bridge_centrality:\n",
    "    bridge = bridge_centrality[id]\n",
    "    info = [id,bridge[\"rum_count\"],bridge[\"nonrum_count\"]]\n",
    "    data.append(info)\n",
    "\n",
    "table.add_rows(data)\n",
    "with open('follower_counts.txt', 'w') as f:\n",
    "    f.write(latextable.draw_latex(table, caption=\"An example table.\", label=\"table:example_table\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow + betweeness + centrality\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/type 1/centrality_2.json\") as file:\n",
    "    bridge_centrality = json.load(file)\n",
    "\n",
    "table = Texttable()\n",
    "table.set_cols_align([\"c\", \"c\", \"c\"])\n",
    "table.set_cols_valign([\"t\", \"t\", \"t\"])\n",
    "table.set_cols_dtype([\"i\",\"e\",\"e\"])\n",
    "\n",
    "data = []\n",
    "for id in bridge_centrality:\n",
    "    bridge = bridge_centrality[id]\n",
    "    info = []\n",
    "    if bridge[\"rumours\"]:\n",
    "        info.append([id,\"betweeness\",\"centrality\"])\n",
    "        info.append([\"rumours\",\"\",\"\"])\n",
    "        \n",
    "        for follower in bridge[\"rumours\"]:\n",
    "            user = (bridge[\"rumours\"])[follower]\n",
    "            info.append([follower,user[\"betweeness\"],user[\"closeness\"]])\n",
    "    \n",
    "    elif bridge[\"non_rumours\"]:\n",
    "        info.append([id,\"betweeness\",\"centrality\"])\n",
    "        info.append([\"non-rumours\",\"\",\"\"])\n",
    "\n",
    "        for follower in bridge[\"non_rumours\"]:\n",
    "            user = (bridge[\"non_rumours\"])[follower]\n",
    "            info.append([follower,user[\"betweeness\"],user[\"closeness\"]])\n",
    "    \n",
    "    data.extend(info)\n",
    "\n",
    "table.add_rows(data)\n",
    "# print(latextable.draw_latex(table, caption=\"An example table.\", label=\"table:example_table\"))\n",
    "with open(head.makePath([figure_path,\"follow\",\"follower_centrality.txt\"]), 'w') as f:\n",
    "    f.write(latextable.draw_latex(table, caption=\"An example table.\", label=\"table:example_table\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of non-rumours followers (sorted)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/follow/type 1/type1.json\") as file:\n",
    "    type1  = json.load(file)\n",
    "\n",
    "nr_followers = {}\n",
    "for id, values in type1.items():\n",
    "    nr_followers[id] = len(values[\"non_rumours\"])\n",
    "\n",
    "nr_followers = dict(sorted(nr_followers.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "header = [\"id\",\"# nodes from non-rumours\"]\n",
    "entries = [header]\n",
    "entries.extend([[x,y] for x,y in nr_followers.items()])\n",
    "\n",
    "cols_align = [\"c\" for i in range(len(header))]\n",
    "cols_valign = [\"t\" for i in range(len(header))]\n",
    "cols_dtype = [\"i\" for i in range(len(header))]\n",
    "\n",
    "head.writeToTxt(head.makePath([figure_path,\"follow\",\"type 1\"]),\"#non-rumour_nodes\",head.makeTable(cols_align,cols_valign,cols_dtype,entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bridges' closeness centrality in non-rumour community \n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/centrality/follow/closeness/non_rumour_closeness.json\") as file:\n",
    "    type1  = json.load(file)\n",
    "\n",
    "closeness = type1[\"bridges\"]\n",
    "\n",
    "closeness = dict(sorted(closeness.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "header = [\"id\",\"closeness centrality\"]\n",
    "entries = [header]\n",
    "entries.extend([[x,y] for x,y in closeness.items()])\n",
    "\n",
    "cols_align = [\"c\" for i in range(len(header))]\n",
    "cols_valign = [\"t\" for i in range(len(header))]\n",
    "cols_dtype = [\"i\",\"e\"]\n",
    "\n",
    "head.writeToTxt(head.makePath([figure_path,\"follow\",\"type 1\"]),\"bridge_nr_closeness\",head.makeTable(cols_align,cols_valign,cols_dtype,entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All reachabel nodes from a bridge\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/centrality/follow/reachable_nr_nodes.json\") as file:\n",
    "    type1  = json.load(file)\n",
    "\n",
    "reachable = {}\n",
    "for id, values in type1.items():\n",
    "    reachable[id] = len(values)\n",
    "\n",
    "reachable = dict(sorted(reachable.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "header = [\"id\",\"# reachable non-rumour nodes\"]\n",
    "entries = [header]\n",
    "entries.extend([[x,y] for x,y in reachable.items()])\n",
    "\n",
    "cols_align = [\"c\" for i in range(len(header))]\n",
    "cols_valign = [\"t\" for i in range(len(header))]\n",
    "cols_dtype = [\"i\",\"i\"]\n",
    "\n",
    "head.writeToTxt(head.makePath([figure_path,\"follow\",\"type 1\"]),\"reachable_nr_nodes\",head.makeTable(cols_align,cols_valign,cols_dtype,entries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follow+React"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/follow+react/type 1/type1.json\") as file:\n",
    "    type1 = json.load(file)\n",
    "\n",
    "header = [\"id\"]+[\"\" for i in range(4)]\n",
    "entries = [header]\n",
    "inter_data = []\n",
    "for id in type1:\n",
    "    inter_data.append(id)\n",
    "    if len(inter_data) == len(header): \n",
    "        entries.append(inter_data)\n",
    "        inter_data = []\n",
    "\n",
    "if len(inter_data) > 0: inter_data.extend([\"\" for i in range(len(header)-len(inter_data))])\n",
    "entries.append(inter_data)\n",
    "\n",
    "cols_align = [\"c\" for i in range(len(header))]\n",
    "cols_valign = [\"t\" for i in range(len(header))]\n",
    "cols_dtype = [\"i\" for i in range(len(header))]\n",
    "\n",
    "head.writeToTxt(head.makePath([figure_path,\"follow+react\",\"type 1\"]),\"type1_ids\",head.makeTable(cols_align,cols_valign,cols_dtype,entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/follow+react/type 3/type 3.json\") as file:\n",
    "    type3 = json.load(file)\n",
    "\n",
    "header = [\"id\"]+[\"\" for i in range(4)]\n",
    "entries = [header]\n",
    "inter_data = []\n",
    "for id in type3:\n",
    "    inter_data.append(id)\n",
    "    if len(inter_data) == len(header): \n",
    "        entries.append(inter_data)\n",
    "        inter_data = []\n",
    "\n",
    "if len(inter_data) > 0: inter_data.extend([\"\" for i in range(len(header)-len(inter_data))])\n",
    "entries.append(inter_data)\n",
    "\n",
    "cols_align = [\"c\" for i in range(len(header))]\n",
    "cols_valign = [\"t\" for i in range(len(header))]\n",
    "cols_dtype = [\"i\" for i in range(len(header))]\n",
    "\n",
    "head.writeToTxt(head.makePath([figure_path,\"follow+react\",\"type 3\"]),\"type3_ids\",head.makeTable(cols_align,cols_valign,cols_dtype,entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follower counts \n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/follow+react/centralities.json\") as file:\n",
    "    bridge_centrality = json.load(file)\n",
    "\n",
    "table = head.get_follower_counts(bridge_centrality,[\"i\",\"i\",\"i\"])\n",
    "with open(head.makePath([head.getRoot(),\"Figures\",\"follow+react\",\"F+R_followers_count.txt\"]), 'w') as f:\n",
    "    f.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table for the number of each type of tweet a bridge tweeted\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/follow+react/type 1/tweets_type.json\") as file:\n",
    "    tweets_type = json.load(file)\n",
    "\n",
    "header = [\"id\",\"\\# source tweet\",\"\\# reply tweet\"]\n",
    "entries = [header]\n",
    "\n",
    "for id, types in tweets_type.items():\n",
    "    inter_data = [id]\n",
    "    for i, num in types.items():\n",
    "        inter_data.append(num)\n",
    "    \n",
    "    entries.append(inter_data)\n",
    "\n",
    "cols_align = [\"c\" for i in range(len(header))]\n",
    "cols_valign = [\"t\" for i in range(len(header))]\n",
    "cols_dtype = [\"i\" for i in range(len(header))]\n",
    "\n",
    "head.writeToTxt(head.makePath([figure_path,\"follow+react\",\"type 1\"]),\"tweets_type\",head.makeTable(cols_align,cols_valign,cols_dtype,entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bridge betweeness\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/follow+react/centralities.json\") as file:\n",
    "    bridge_centrality = json.load(file)\n",
    "\n",
    "table = head.get_bridge_betweeness(bridge_centrality,[\"i\",\"e\"])\n",
    "with open(head.makePath([head.getRoot(),\"Figures\",\"follow+react\",\"F+R_bridge_betweeness.txt\"]), 'w') as f:\n",
    "    f.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Followers betweeness and centrality\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/follow+react/centralities.json\") as file:\n",
    "    bridge_centrality = json.load(file)\n",
    "\n",
    "table = head.get_followers_centralities(bridge_centrality,[\"i\",\"e\",\"e\"])\n",
    "with open(head.makePath([head.getRoot(),\"Figures\",\"follow+react\",\"F+R_followers_centralities.txt\"]), 'w') as f:\n",
    "    f.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/threads/en/charliehebdo/552783667052167168/reactions/552785374507175936.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "head.writeToJSON(head.makePath([head.getRoot(),\"Figures\"]),\"example_tweet_structure\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do any non-rumours follows bridges \n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/community_detection/follow/with_bridges.json\") as file:\n",
    "    F_communities = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/who-follows-whom.json\") as file:\n",
    "    follows = json.load(file)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/who_followed_by_whom.json\") as file:\n",
    "    followed_by = json.load(file)\n",
    "\n",
    "for id in F_communities[\"non_rumours\"]:\n",
    "    try: f = follows[id]; fb = followed_by[id]\n",
    "    except: continue\n",
    "\n",
    "    f = [i for i in f if i in F_communities[\"bridges\"] or i in F_communities[\"rumours\"]]\n",
    "    fb = [i for i in f if i in F_communities[\"non_rumours\"]]\n",
    "\n",
    "    if len(f) and len(fb): print(id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of non-rumours followers (sorted)\n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/bridges/follow+react/type 1/type1.json\") as file:\n",
    "    type1  = json.load(file)\n",
    "\n",
    "nr_followers = {}\n",
    "for id, values in type1.items():\n",
    "    nr_followers[id] = len(values[\"non_rumours\"])\n",
    "\n",
    "nr_followers = dict(sorted(nr_followers.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "header = [\"id\",\"# nodes from non-rumours\"]\n",
    "entries = [header]\n",
    "entries.extend([[x,y] for x,y in nr_followers.items()])\n",
    "\n",
    "cols_align = [\"c\" for i in range(len(header))]\n",
    "cols_valign = [\"t\" for i in range(len(header))]\n",
    "cols_dtype = [\"i\" for i in range(len(header))]\n",
    "\n",
    "head.writeToTxt(head.makePath([figure_path,\"follow+react\",\"type 1\"]),\"#non-rumour_nodes\",head.makeTable(cols_align,cols_valign,cols_dtype,entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bridges' closeness centrality in non-rumour community \n",
    "\n",
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/centrality/follow+react/closeness/nonrumours+bridges.json\") as file:\n",
    "    type1  = json.load(file)\n",
    "\n",
    "closeness = type1[\"bridges\"]\n",
    "\n",
    "closeness = dict(sorted(closeness.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "header = [\"id\",\"closeness centrality\"]\n",
    "entries = [header]\n",
    "entries.extend([[x,y] for x,y in closeness.items()])\n",
    "\n",
    "cols_align = [\"c\" for i in range(len(header))]\n",
    "cols_valign = [\"t\" for i in range(len(header))]\n",
    "cols_dtype = [\"i\",\"e\"]\n",
    "\n",
    "head.writeToTxt(head.makePath([figure_path,\"follow+react\",\"type 1\"]),\"bridge_nr_closeness\",head.makeTable(cols_align,cols_valign,cols_dtype,entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yu-hung/Downloads/pheme-rumour-scheme-dataset/centrality/follow+react/reachable_nr_nodes.json\") as file:\n",
    "    type1  = json.load(file)\n",
    "\n",
    "reachable = {}\n",
    "for id, values in type1.items():\n",
    "    reachable[id] = len(values)\n",
    "\n",
    "reachable = dict(sorted(reachable.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "header = [\"id\",\"# reachable non-rumour nodes\"]\n",
    "entries = [header]\n",
    "entries.extend([[x,y] for x,y in reachable.items()])\n",
    "\n",
    "cols_align = [\"c\" for i in range(len(header))]\n",
    "cols_valign = [\"t\" for i in range(len(header))]\n",
    "cols_dtype = [\"i\",\"i\"]\n",
    "\n",
    "head.writeToTxt(head.makePath([figure_path,\"follow+react\",\"type 1\"]),\"reachable_nr_nodes\",head.makeTable(cols_align,cols_valign,cols_dtype,entries))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
